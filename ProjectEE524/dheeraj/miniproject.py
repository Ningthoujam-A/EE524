# -*- coding: utf-8 -*-
"""miniproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z3LnS3KI9s3_lPp6Y8HnDPcdZVhphEIi
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
sns.set(color_codes=True)
import scipy.linalg as la

"""# **loading data**"""

url='http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'
dataset=pd.read_csv(url,header=None)

X = dataset.iloc[:, 2:32].values
Y = dataset.iloc[:, 1].values

print("Cancer data set dimensions : {}".format(dataset.shape))

"""# **viewing data**"""

dataset.head()

"""# **labeling data**"""

#Encoding categorical data values
from sklearn.preprocessing import LabelEncoder
labelencoder_Y = LabelEncoder()
Y = labelencoder_Y.fit_transform(Y)
# print(Y)

"""# **data visualizing**"""

colormap=np.array(['r','g'])
plt.figure(1)

plt.scatter(X[:,2],X[:, 3],c=colormap[Y])
labels=np.unique(Y)
plt.legend(labels)

plt.show()

plt.scatter(X[:,0],X[:,3],c=colormap[Y])
labels=np.unique(Y)
plt.legend(labels)

plt.show()

"""# principal component analysis(PCA)

# **feature means**
"""

mean_vectors=list()
for i in range(30):
  means=X[:,i].mean()
  mean_vectors.append(means)

"""# **#definning covarince**"""

#definning covarince
def covar(x1,x2,m1,m2):
    l=len(x1)
    sum=0
    for i in range(l):
        sum=sum+((x1[i]-m1)*(x2[i]-m2))
    return sum/float(l-1)

"""# **finding covariance matrix**"""

#finding covariance matrix
co_mat =np.zeros((30,30), dtype=np.float64)
for i in range(30):
    for j in range(30):
        co_mat[i][j]=covar(X[0:][i],X[0:][j],mean_vectors[i],mean_vectors[j])
# print(co_mat)

"""**#finding eigen value and eigen vectors**"""

#finding eigen value and eigen vectors
eigen_val=la.eig(co_mat)[0]
eigen_vectors=la.eig(co_mat)[1]
# print(eigen_vectors)
#print(eigen_val)

v=eigen_vectors[:,0:3]

"""**#new feature space**"""

y=np.dot(X-mean_vectors,v)
# print(y.shape)

"""**data visualization after pca**"""

plt.scatter(y[:,1],y[:,2],c=colormap[Y])
labels=np.unique(Y)
plt.legend(labels)

plt.show()

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(y, Y, test_size = 0.25, random_state = 0)

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train,Y_train)
pred=classifier.predict(X_test)

"""# **accuracy**"""

def accuracy(x,y):
    acc=np.sum(x==y)/len(x)
    return acc

print(accuracy(pred,Y_test))

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
accuracy_list=list()
for i in range(1,31):
  v=eigen_vectors[:,0:i]
  y=np.dot(X-mean_vectors,v)
  X_train, X_test, Y_train, Y_test = train_test_split(y, Y, test_size = 0.25, random_state = 0)
  classifier = GaussianNB()
  classifier.fit(X_train,Y_train)
  pred=classifier.predict(X_test)
  acc=accuracy(pred,Y_test) 
  accuracy_list.append(acc)
# print(accuracy_list)

n = np.arange(30)
plt.plot(n,accuracy_list)
plt.xlabel("number of first n highest magnitude eigen value")
plt.ylabel("accuracy")
plt.show

from sklearn.linear_model import LogisticRegression

accuracy_list=list()
for i in range(1,31):
  v=eigen_vectors[:,0:i]
  y=np.dot(X-mean_vectors,v)
  X_train, X_test, Y_train, Y_test = train_test_split(y, Y, test_size = 0.25, random_state = 0)
  classifier = LogisticRegression(random_state = 0)
  classifier.fit(X_train,Y_train)
  pred=classifier.predict(X_test)
  acc=accuracy(pred,Y_test) 
  accuracy_list.append(acc)

n = np.arange(30)
plt.plot(n,accuracy_list)
plt.xlabel("number of first n highest magnitude eigen value")
plt.ylabel("accuracy")
plt.show

from sklearn.svm import SVC

accuracy_list=list()
for i in range(1,31):
  v=eigen_vectors[:,0:i]
  y=np.dot(X-mean_vectors,v)
  X_train, X_test, Y_train, Y_test = train_test_split(y, Y, test_size = 0.25, random_state = 0)
  classifier = SVC(kernel = 'linear', random_state = 0)
  classifier.fit(X_train,Y_train)
  pred=classifier.predict(X_test)
  acc=accuracy(pred,Y_test) 
  accuracy_list.append(acc)
# print(accuracy_list)

n = np.arange(30)
plt.plot(n,accuracy_list)
plt.xlabel("number of first n highest magnitude eigen value")
plt.ylabel("accuracy")
plt.show

from sklearn.neighbors import KNeighborsClassifier

accuracy_list=list()
for i in range(1,31):
  v=eigen_vectors[:,0:i]
  y=np.dot(X-mean_vectors,v)
  X_train, X_test, Y_train, Y_test = train_test_split(y, Y, test_size = 0.25, random_state = 0)
  classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
  classifier.fit(X_train,Y_train)
  pred=classifier.predict(X_test)
  acc=accuracy(pred,Y_test) 
  accuracy_list.append(acc)
# print(accuracy_list)

n = np.arange(30)
plt.plot(n,accuracy_list)
plt.xlabel("number of first n highest magnitude eigen value")
plt.ylabel("accuracy")
plt.show

"""# **linear discriminant analysis**

**separating class**
"""

X1 = dataset.iloc[:, 1:32].values
W1=list()
W2=list()
for i in range(len(X1[:,0])):
  if X1[:,0][i]=='M':
    W1.append(X1[i,1:])
    
  else:
    W2.append(X1[i,1:])
W1=np.array(W1) 
W2=np.array(W2)

"""**means of each class**"""

mu1=list()
mu2=list()
for i in range(30):
  means=W1[:,i].mean()
  mu1.append(means)
for i in range(30):
  means=W2[:,i].mean()
  mu2.append(means)

"""**covarience matrix of class 1**"""

S1 =np.zeros((30,30), dtype=np.float64)
for i in range(30):
    for j in range(30):
        S1[i][j]=covar(W1[0:][i],W2[0:][j],mu1[i],mu1[j])

"""**covarience matrix of class 2**"""

S2=np.zeros((30,30), dtype=np.float64)
for i in range(30):
    for j in range(30):
        S2[i][j]=covar(W2[0:][i],W2[0:][j],mu2[i],mu2[j])

mu1=np.array([mu1])
mu2=np.array([mu2])
m=mu1-mu2

m1=m.transpose()
# print(m.shape)
# print(m1.shape)

"""**within class scatter and between class scatter matrix**"""

Sw=S1+S2
Sb=np.dot(m1,m)
Sw_inv=np.linalg.inv(Sw)
Sw_inv_Sb=np.dot(Sw_inv,Sb)

"""**eigen value and eigen vector**"""

eigen_val=la.eig(Sw_inv_Sb)[0]
eigen_vectors=la.eig(Sw_inv_Sb)[1]

"""sorting eigen value and corresponding eigen vectors"""

pairs = [(np.abs(eigen_val[i]), eigen_vectors[:,i]) for i in range(len(eigen_val))]
pairs = sorted(pairs, key=lambda x: x[0], reverse=True)

pairs=np.array(pairs)

"""taking first n eigen vectors and creating transform matrix"""

for i in range(1,8):
  w_=list()
  for j in range(i):
    w=np.hstack((pairs[j-1][1].reshape(30,1))).real
    w_.append(w)
    w_1=np.array(w_).transpose()
    X_lda = np.array(X.dot(w_1))

"""finding accuracy for logistic regression"""

from sklearn.linear_model import LogisticRegression

X_train, X_test, Y_train, Y_test = train_test_split(X_lda, Y, test_size = 0.25, random_state = 0)
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train,Y_train)
pred=classifier.predict(X_test)
acc=accuracy(pred,Y_test) 
print(acc)

"""accuracy for Naive Bayes classifier"""

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train,Y_train)
pred=classifier.predict(X_test)
acc=accuracy(pred,Y_test)
print(acc)

"""using SVM"""

classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train,Y_train)
pred=classifier.predict(X_test)
acc=accuracy(pred,Y_test) 

print(acc)

"""using KNN"""

classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier.fit(X_train,Y_train)
pred=classifier.predict(X_test)
acc=accuracy(pred,Y_test) 
print(acc)

plt.scatter(X_lda[:,1],X_lda[:,2],c=colormap[Y])
labels=np.unique(Y)
plt.legend(labels)

plt.show()